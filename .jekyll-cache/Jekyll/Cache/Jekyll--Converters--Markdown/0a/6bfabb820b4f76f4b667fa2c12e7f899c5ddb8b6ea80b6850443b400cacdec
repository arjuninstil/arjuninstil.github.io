I"zh<h1 id="basic-topic-modelling">Basic Topic Modelling</h1>

<h2 id="topic-detection">Topic detection</h2>

<p>Topic detection is a way to extract relevant information from texts. The topic is a set of words (from the text) having particular relevance in terms of probability. They apper to be words that characterize the topics (one or more) discussed in the documents.</p>

<p><strong>Definitions:</strong></p>

<ul>
  <li>Document: A single text, paragraph or even tweet to be classified</li>
  <li>Word/Term: a single component of a document</li>
  <li>Topic: a set of words describing a group (cluster) of documents</li>
</ul>

<p><strong>each document usually is as a mixture of several topics</strong></p>

<h3 id="mixture-of-topics">Mixture of topics</h3>
<p>Suppose you have the following set of sentences:</p>

<p>I like to eat broccoli and bananas.
I ate a banana and spinach smoothie for breakfast.
Chinchillas and kittens are cute.
My sister adopted a kitten yesterday.
Look at this cute hamster munching on a piece of broccoli.
A model such as LDA will produce an classification such as the following:</p>

<ul>
  <li>Sentences 1 and 2: 100% Topic A</li>
  <li>Sentences 3 and 4: 100% Topic B</li>
  <li>Sentence 5: 60% Topic A, 40% Topic B</li>
</ul>

<p>Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)</p>

<p>Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)</p>

<h3 id="methodologies">Methodologies</h3>

<ul>
  <li>latent dirichlet allocation (lda)</li>
  <li>Non negative matrix factorization</li>
  <li>Clustering</li>
</ul>

<p><strong>latent dirichlet allocation (lda)</strong></p>

<p>It’s a complex mathematical model (based on Bayesian statistics and Dirichlet and Multinomial distributions) to establish the words in a set of documents that are the most representative. The starting point is definining a</p>

<ul>
  <li>fixed number of topics K</li>
  <li>to each topic k we associate a probability p = p(k,w) i.e. the probability of seeing the topic k given the set of words w in the document d</li>
  <li>to each topic k we associate a probability s = s(k,d) i.e. the probability of a k topic belonging to the document d. The distribution s represents the mixture of topics related to d</li>
  <li>A word in the document is picked by randomly extracting from a topic and from a document according to s and p distributions</li>
  <li>An optimization is performed fitting the s,p distributions to the actual distribution of words in the documents.</li>
</ul>

<p><strong>Non negative matrix factorization</strong></p>

<ul>
  <li><strong>V</strong>  is the matrix representing all documents</li>
  <li><strong>H</strong> is the matrix representing documents given the topics</li>
  <li><strong>W</strong> is the matrix representing the topics</li>
</ul>

<p>the factorization is made using objective functions such as <em>Frobenius Norm</em></p>

<h3 id="main-features">Main features</h3>

<p><strong>LDA</strong></p>

<ul>
  <li>Slow method</li>
  <li>Quite accurate for large corpora where each document is a mixture of topics</li>
  <li>Most adopted</li>
</ul>

<p><strong>NMF</strong></p>

<ul>
  <li>Fast method</li>
  <li>Accurate with small corpora (i.e. tweets) or tweets with no mixture of topics</li>
  <li>not commonly adopted</li>
</ul>

<h2 id="hands-on">Hands on</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="kn">import</span> <span class="nn">re</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span><span class="p">,</span> <span class="n">LatentDirichletAllocation</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
</code></pre></div></div>

<p><strong>get the corpus</strong></p>

<p><em>20 newsgroup</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">newsgroups</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s">'headers'</span><span class="p">,</span> <span class="s">'footers'</span><span class="p">,</span> <span class="s">'quotes'</span><span class="p">))</span>
<span class="n">docs_raw</span> <span class="o">=</span> <span class="n">newsgroups</span><span class="p">.</span><span class="n">data</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">docs_raw</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stops_it</span> <span class="o">=</span> <span class="n">stopwords</span><span class="p">.</span><span class="n">words</span><span class="p">(</span><span class="s">'italian'</span><span class="p">)</span>
<span class="n">stops_en</span> <span class="o">=</span> <span class="n">stopwords</span><span class="p">.</span><span class="n">words</span><span class="p">(</span><span class="s">'english'</span><span class="p">)</span>

<span class="n">translator</span> <span class="o">=</span> <span class="nb">str</span><span class="p">.</span><span class="n">maketrans</span><span class="p">(</span><span class="s">' '</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">)</span> <span class="c1">## remove the punctuation
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">minimumSize</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span><span class="n">llen</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
    <span class="c1">## remove words smaller than llen chars
</span>    <span class="n">tks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="k">if</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">llen</span><span class="p">):</span>
            <span class="n">tks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tks</span>

<span class="k">def</span> <span class="nf">removeStops</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span><span class="n">stops</span> <span class="o">=</span> <span class="n">stops_it</span><span class="p">):</span>
    <span class="c1"># remove stop words
</span>    <span class="n">remains</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
        <span class="k">if</span><span class="p">(</span><span class="n">t</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stops</span><span class="p">):</span>
            <span class="n">remains</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">remains</span>

<span class="k">def</span> <span class="nf">processText</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1">## tokenizer with stop words removal and minimum size 
</span>    <span class="n">tks</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">tks</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">.</span><span class="n">translate</span><span class="p">(</span><span class="n">translator</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tks</span><span class="p">]</span> <span class="c1">## remove the punctuation
</span>    <span class="n">tks</span> <span class="o">=</span> <span class="n">minimumSize</span><span class="p">(</span><span class="n">tks</span><span class="p">)</span>
    <span class="n">tks</span> <span class="o">=</span> <span class="n">removeStops</span><span class="p">(</span><span class="n">tks</span><span class="p">,</span><span class="n">stops_en</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tks</span>
</code></pre></div></div>

<h3 id="tfidf-vectorizer">TFIDF vectorizer</h3>

<p>It transforms each word in the D documents in a sparse matrix representing a normalized frequency of each word in each document.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_features</span> <span class="o">=</span> <span class="mi">1000</span> 
<span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_df</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                   <span class="n">max_features</span><span class="o">=</span><span class="n">n_features</span><span class="p">,</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">processText</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>n_features</strong> it’s the number of individual ters from the corpus to use (notice that rarely a language by humans uses more than few thousands of distinct words ). Having a large dataset it is safe to use large number for n_features, for short corpus n_features must be non large</p>

<p><strong>max_df</strong> is the probability at which the more probable words must be removed (removes the most common words)</p>

<p><strong>min_df</strong> removes the words appearing less than 2 times in the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">corpusT</span> <span class="o">=</span> <span class="n">docs_raw</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">]</span> <span class="c1">## let's use the first 500 documents
</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpusT</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tfidf</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;500x1000 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 12953 stored elements in Compressed Sparse Row format&gt;
</code></pre></div></div>

<p><strong>associate names (words) to each feature</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tfidf_feature_names</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="p">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>LDA</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_topics</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                <span class="n">learning_method</span> <span class="o">=</span> <span class="s">'batch'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">LatentDirichletAllocation</span><span class="p">(</span><span class="n">n_topics</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,
                          evaluate_every=-1, learning_decay=0.7,
                          learning_method='batch', learning_offset=10.0,
                          max_doc_update_iter=100, max_iter=10,
                          mean_change_tol=0.001, n_components=20, n_jobs=None,
                          perp_tol=0.1, random_state=None,
                          topic_word_prior=None, total_samples=1000000.0,
                          verbose=0)
</code></pre></div></div>

<ul>
  <li><strong>n_topics</strong> is somehow arbitrary.</li>
  <li><strong>max_iter</strong> stops the iteration after maximum 10</li>
  <li><strong>learning method</strong> is usually online but can be also batch (slower) when all data are processed at time</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lda</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf</span><span class="p">)</span> <span class="c1">## fit the model
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,
                          evaluate_every=-1, learning_decay=0.7,
                          learning_method='batch', learning_offset=10.0,
                          max_doc_update_iter=100, max_iter=10,
                          mean_change_tol=0.001, n_components=20, n_jobs=None,
                          perp_tol=0.1, random_state=None,
                          topic_word_prior=None, total_samples=1000000.0,
                          verbose=0)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mostImportantWordsPerTopic</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span><span class="n">topic</span><span class="p">,</span><span class="n">n_top_words</span><span class="p">):</span>
    <span class="n">mwords</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sort_topic</span> <span class="o">=</span> <span class="n">topic</span><span class="p">.</span><span class="n">argsort</span><span class="p">()</span>
    <span class="n">mw</span> <span class="o">=</span> <span class="n">sort_topic</span><span class="p">[:</span><span class="o">-</span><span class="n">n_top_words</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1">## reversed list    
</span>    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">mw</span><span class="p">:</span>
        <span class="n">mwords</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature_names</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">mwords</span>
        

<span class="k">def</span> <span class="nf">print_top_words</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">n_top_words</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">topic_idx</span><span class="p">,</span> <span class="n">topic</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">components_</span><span class="p">):</span>
        
        <span class="n">most_important_words</span> <span class="o">=</span> <span class="n">mostImportantWordsPerTopic</span><span class="p">(</span><span class="n">feature_names</span><span class="p">,</span><span class="n">topic</span><span class="p">,</span><span class="n">n_top_words</span><span class="p">)</span>

        <span class="n">message</span> <span class="o">=</span> <span class="s">"Topic #%d: "</span> <span class="o">%</span> <span class="n">topic_idx</span>
        <span class="n">message</span> <span class="o">+=</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">most_important_words</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Printing the topics</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_top_words</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">print_top_words</span><span class="p">(</span><span class="n">lda</span><span class="p">,</span> <span class="n">tfidf_feature_names</span><span class="p">,</span> <span class="n">n_top_words</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Topic #0: national today defined wrong reference son mail network appreciated information
Topic #1: simms use machine bbs hardware sort love small congress anyway
Topic #2: window reply cars write image technology helps windows fine long
Topic #3: would know one people think could get good use really
Topic #4: possible yes digital morality tiff audio turn live western entire
Topic #5: year seen last great first memory problem house believe file
Topic #6: years water speeds version plus current drugs needed starters faster
Topic #7: team next small cameras crime large books looks battery rates
Topic #8: israel launch space would say moon less notes water server
Topic #9: list questions company got card avoid email also wondering thanks
Topic #10: armenian less look turkish genocide problems special understood tower color
Topic #11: program true truth ideas clipper pro read files send gear
Topic #12: test max talking maybe eternal water playing cut heat assuming
Topic #13: application jews modem problem regardless insurance reply scope left father
Topic #14: prevent name abc spacecraft problem radio found land worked sale
Topic #15: box somebody american neither thank land real force hell anyone
Topic #16: god two find would someone comes cost information different like
Topic #17: start windows place controller however research users feel life unless
Topic #18: print thanks email motif problem reserve ibm shuttle one basically
Topic #19: things apparently weeks serial received crypto purchased yesterday change whole
</code></pre></div></div>

<p><strong>NMF</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nmf</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">alpha</span><span class="o">=</span><span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">tfidf</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>parameters</strong></p>

<p>NMF is basically free of parameters :).</p>

<ul>
  <li>alpha : regolarization parameter (used to smooth the frequencies and to improve the fit)</li>
  <li>l1_ratio : regolarization parameter (used to smooth the frequencies and to improve the fit)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_top_words</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">print_top_words</span><span class="p">(</span><span class="n">nmf</span><span class="p">,</span> <span class="n">tfidf_feature_names</span><span class="p">,</span> <span class="n">n_top_words</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Topic #0: would get one think people like know much could may
Topic #1: thanks please email address list advance information available net anybody
Topic #2: use simms memory machine mac could several need answer work
Topic #3: year last yes old three years great game time mask
Topic #4: problem found used light known check error however think running
Topic #5: file printer print manager like another port instead name driver
Topic #6: window box control want left get option application manager upper
Topic #7: looking card working must email condition mail appreciated buy spend
Topic #8: problems pain obvious gave anybody following ask sure also cars
Topic #9: possible yes phone crypto interest invalid fire eternal understanding soviet
Topic #10: things apparently worse like also little exactly seem basically reality
Topic #11: post message product real research feel could server sorry error
Topic #12: program windows files april run software microsoft image code version
Topic #13: lost services man new think hand take nothing called considered
Topic #14: anyone find know would information etc hello good like obvious
Topic #15: land jews appears power jewish man worked right purpose part
Topic #16: water steam heat hot used oil israel cup engine rather
Topic #17: armenian turkish genocide armenians xsoviet turks russian muslim people kurds
Topic #18: send asking reply want following new sale clock included server
Topic #19: controller esdi ram help ide card scsi need bios appreciated
</code></pre></div></div>

<h3 id="bonus-visualization">Bonus visualization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">pyLDAvis</span>
<span class="kn">import</span> <span class="nn">pyLDAvis.sklearn</span>
<span class="n">pyLDAvis</span><span class="p">.</span><span class="n">enable_notebook</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pyLDAvis</span><span class="p">.</span><span class="n">__version__</span> <span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="n">__version__</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('2.1.1', '0.24.2')
</code></pre></div></div>

<p><strong>default visualization of topics and frequency in a multidimensional space</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="n">pyLDAvis</span><span class="p">.</span><span class="n">sklearn</span><span class="p">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">lda</span><span class="p">,</span> <span class="n">tfidf</span><span class="p">,</span> <span class="n">tfidf_vectorizer</span><span class="p">)</span>
<span class="n">pyLDAvis</span><span class="p">.</span><span class="n">save_html</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s">'lda.html'</span><span class="p">)</span>
</code></pre></div></div>

<iframe src="/img/posts/Basic-Topic-Modelling/interactive_topic.html" height="800px" width="120%"></iframe>

:ET